{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5ca911-689a-4411-8ebb-756ca00a5cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pyspark in c:\\users\\s560092\\big-data-venv\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\s560092\\big-data-venv\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebff26cb-174d-46f0-b4c9-bfdfc2b42b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigBasket Analysis\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea96e831-d6fe-4f24-8a94-375d6752bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the provided path\n",
    "file_path = \"C:/Users/S560092/Downloads/BigBasket Products.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71283e64-6ca1-4783-839a-af8bbdb9825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-----------------+----------+------------+--------------------+------+--------------------+\n",
      "|               index|             product|            category|        sub_category|            brand|sale_price|market_price|                type|rating|         description|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------+----------+------------+--------------------+------+--------------------+\n",
      "|                   1|Garlic Oil - Vege...|    Beauty & Hygiene|           Hair Care|Sri Sri Ayurveda |       220|         220|    Hair Oil & Serum|   4.1|This Product cont...|\n",
      "|                   2|Water Bottle - Or...|Kitchen, Garden &...|Storage & Accesso...|       Mastercook|       180|         180|Water & Fridge Bo...|   2.3|Each product is m...|\n",
      "|                   3|Brass Angle Deep ...|Cleaning & Household|         Pooja Needs|              Trm|       119|         250|     Lamp & Lamp Oil|   3.4|A perfect gift fo...|\n",
      "|                   4|Cereal Flip Lid C...|Cleaning & Household|Bins & Bathroom Ware|           Nakoda|       149|         176|Laundry, Storage ...|   3.7|Multipurpose cont...|\n",
      "|The Nakoda contai...| this container w...| cleaning and mai...|                NULL|             NULL|      NULL|        NULL|                NULL|  NULL|                NULL|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------+----------+------------+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows of the dataset\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0e11bb-40d4-4ca4-ac20-58b7eb79b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             product|count|\n",
      "+--------------------+-----+\n",
      "|                NULL| 5379|\n",
      "| tricks & more vi...|   38|\n",
      "| on occasion manu...|   32|\n",
      "|Turmeric Powder/A...|   26|\n",
      "| want a condition...|   25|\n",
      "| now available in...|   21|\n",
      "| the actually pro...|   19|\n",
      "| keeping the hair...|   19|\n",
      "|          phosphorus|   17|\n",
      "|           cocktails|   16|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# goal1  Top 10 Most Reviewed Products (By count of products per category)\n",
    "df.groupBy(\"category\").count().orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c42b257-348f-4c44-b882-5b0db5e44c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- sale_price: string (nullable = true)\n",
      " |-- market_price: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fb6ce9c-c472-40e5-9c06-3820685f733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert rating column to DoubleType\n",
    "df = df.withColumn(\"rating\", col(\"rating\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7da1185-f406-49e3-80fc-55bbcc007f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|rating|count|\n",
      "+------+-----+\n",
      "|  NULL|18304|\n",
      "|   1.0|  387|\n",
      "|   1.2|    2|\n",
      "|   1.3|    9|\n",
      "|   1.4|    6|\n",
      "|   1.5|   32|\n",
      "|   1.6|    3|\n",
      "|   1.7|   22|\n",
      "|   1.8|   22|\n",
      "|   1.9|    4|\n",
      "|   2.0|  237|\n",
      "|   2.1|   10|\n",
      "|   2.2|   24|\n",
      "|   2.3|   94|\n",
      "|   2.4|   29|\n",
      "|   2.5|  132|\n",
      "|   2.6|   58|\n",
      "|   2.7|  115|\n",
      "|   2.8|  125|\n",
      "|   2.9|   79|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Goal 3: Rating Distribution\n",
    "rating_distribution = df.groupBy(\"rating\").count().orderBy(\"rating\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cc7b59d-65e0-44ec-b902-0ab2a1e18900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|             product|avg(rating)|\n",
      "+--------------------+-----------+\n",
      "|            Guar Gum|        5.0|\n",
      "|Eau-De-Mehfil Eau...|        5.0|\n",
      "|Organic Shield - ...|        5.0|\n",
      "|Lemon and Active ...|        5.0|\n",
      "|Vitamin D Gummies...|        5.0|\n",
      "|Coconut & Olive O...|        5.0|\n",
      "|Glass Belleza Bow...|        5.0|\n",
      "|Marvel Avengers P...|        5.0|\n",
      "|Fresh Start Water...|        5.0|\n",
      "|Borosilicate Glas...|        5.0|\n",
      "+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Goal 2: Top 10 Products by Average Rating\n",
    "top_rated_products = df.groupBy(\"product\").avg(\"rating\").orderBy(\"avg(rating)\", ascending=False).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f557f45-0eb1-4fbb-a948-5dbdd8149517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- sale_price: string (nullable = true)\n",
      " |-- market_price: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema to find any date column\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd880a96-d00d-476d-b3f3-a55d00c40ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|month|count|\n",
      "+-----+-----+\n",
      "| NULL| 9677|\n",
      "|    1|   10|\n",
      "|    2|   10|\n",
      "|    3|   10|\n",
      "|    4|   10|\n",
      "|    5|   10|\n",
      "|    6|   10|\n",
      "|    7|   10|\n",
      "|    8|   10|\n",
      "|    9|   10|\n",
      "|   10|   10|\n",
      "|   11|   10|\n",
      "|   12|   10|\n",
      "|   13|   10|\n",
      "|   14|   10|\n",
      "|   15|   10|\n",
      "|   16|   10|\n",
      "|   17|   10|\n",
      "|   18|   10|\n",
      "|   19|   10|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, ceil\n",
    "\n",
    "# Create synthetic month based on the index or product order.\n",
    "# Assume we divide the index into groups representing months or periods.\n",
    "df_with_month = df.withColumn(\"month\", ceil(col(\"index\") / 10))  # Example: Index 1-10 => Month 1, 11-20 => Month 2\n",
    "\n",
    "# Group by the synthetic 'month' and count the products in each 'month'.\n",
    "monthly_trends = df_with_month.groupBy(\"month\").count().orderBy(\"month\")\n",
    "\n",
    "# Show the result.\n",
    "monthly_trends.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c373f9ac-5148-433b-be65-f65ab4cae778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting nltk>=3.8 (from textblob)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk>=3.8->textblob)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk>=3.8->textblob)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\s560092\\big-data-venv\\lib\\site-packages (from nltk>=3.8->textblob) (2024.11.6)\n",
      "Collecting tqdm (from nltk>=3.8->textblob)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\s560092\\big-data-venv\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 626.3/626.3 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.3 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, joblib, click, nltk, textblob\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 textblob-0.18.0.post0 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd74b231-2db8-4026-8d86-2be4e0e3cdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\S560092\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\S560092\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be48eb61-e43f-4f92-bc10-1dff3d31eb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     product sentiment\n",
      "0  Product A  Positive\n",
      "1  Product B  Negative\n",
      "2  Product C   Neutral\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Goal 5: Sentiment Analysis using Keyword-Based Method\n",
    "\n",
    "# Sample data (replace with your actual DataFrame)\n",
    "data = {'product': ['Product A', 'Product B', 'Product C'],\n",
    "        'description': ['I love this product', 'This is terrible', 'It works as expected']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define lists of positive and negative keywords\n",
    "positive_words = ['love', 'great', 'excellent', 'good', 'amazing', 'fantastic', 'awesome']\n",
    "negative_words = ['terrible', 'bad', 'horrible', 'awful', 'disappointing', 'poor', 'worse']\n",
    "\n",
    "# Function to classify sentiment based on keywords\n",
    "def get_sentiment(text):\n",
    "    # Convert text to lower case to make the comparison case-insensitive\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Initialize sentiment score (0 for neutral, 1 for positive, -1 for negative)\n",
    "    sentiment_score = 0\n",
    "    \n",
    "    # Check for positive and negative words in the text\n",
    "    for word in positive_words:\n",
    "        if word in text:\n",
    "            sentiment_score += 1\n",
    "    \n",
    "    for word in negative_words:\n",
    "        if word in text:\n",
    "            sentiment_score -= 1\n",
    "    \n",
    "    # Classify sentiment\n",
    "    if sentiment_score > 0:\n",
    "        return 'Positive'\n",
    "    elif sentiment_score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to the 'description' column\n",
    "df['sentiment'] = df['description'].apply(get_sentiment)\n",
    "\n",
    "# Show the results\n",
    "print(df[['product', 'sentiment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34ad95bc-f6d0-4a45-8f17-c72f451b0df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      category  count\n",
      "0     Clothing      2\n",
      "1  Electronics      2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data (replace with your actual DataFrame)\n",
    "data = {'product': ['Product A', 'Product B', 'Product C', 'Product D'],\n",
    "        'category': ['Electronics', 'Clothing', 'Electronics', 'Clothing']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Goal 6: Product Categories Popularity using Pandas\n",
    "\n",
    "# Group by category and count the occurrences\n",
    "category_popularity = df.groupby('category').size().reset_index(name='count')\n",
    "\n",
    "# Sort by count in descending order and display top 10\n",
    "category_popularity_sorted = category_popularity.sort_values(by='count', ascending=False).head(10)\n",
    "\n",
    "# Show the result\n",
    "print(category_popularity_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94b3ad2f-2f22-4c81-bca9-fc3d872e61ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m length, col\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Goal 7: Word Count for Product Descriptions in PySpark\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df_with_word_count \u001b[38;5;241m=\u001b[39m \u001b[43mdf_spark\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m, length(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Show the result\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df_with_word_count\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, col\n",
    "\n",
    "# Goal 7: Word Count for Product Descriptions in PySpark\n",
    "\n",
    "df_with_word_count = df_spark.withColumn('word_count', length(col('description')).alias('word_count'))\n",
    "\n",
    "# Show the result\n",
    "df_with_word_count.select('product', 'description', 'word_count').show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99105c37-077d-4497-a03a-fdbfc7a10c4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `description` cannot be resolved. Did you mean one of the following? [`product`, `category`].;\n'Project [product#275, category#276, length('description) AS word_count#280]\n+- LogicalRDD [product#275, category#276], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m df_spark \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Now you can proceed with Goal 7\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df_with_word_count \u001b[38;5;241m=\u001b[39m \u001b[43mdf_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Show the result\u001b[39;00m\n\u001b[0;32m     14\u001b[0m df_with_word_count\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32m~\\big-data-venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5176\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5175\u001b[0m     )\n\u001b[1;32m-> 5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\big-data-venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\big-data-venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `description` cannot be resolved. Did you mean one of the following? [`product`, `category`].;\n'Project [product#275, category#276, length('description) AS word_count#280]\n+- LogicalRDD [product#275, category#276], false\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length, col\n",
    "\n",
    "# Start a Spark session (if not already started)\n",
    "spark = SparkSession.builder.appName(\"Word Count\").getOrCreate()\n",
    "\n",
    "# Assuming df is a Pandas DataFrame, convert it to a PySpark DataFrame\n",
    "df_spark = spark.createDataFrame(df)\n",
    "\n",
    "# Now you can proceed with Goal 7\n",
    "df_with_word_count = df_spark.withColumn('word_count', length(col('description')).alias('word_count'))\n",
    "\n",
    "# Show the result\n",
    "df_with_word_count.select('product', 'description', 'word_count').show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cd4aa99-7547-447f-a751-a23dfc702838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['product', 'category']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the column names of the DataFrame\n",
    "df_spark.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2702c-aff2-43e6-9958-10ab45b9b3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
